{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install score_models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models import NCSNpp, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape of the input is `[B, C]`, where `B` is an unspecified batch size.\n",
    "- `C` is the number of channels of the input vector.\n",
    "- `units` is the width of the hidden layers.\n",
    "- `layers` is the number of hidden layers, not counting the attention bottleneck.\n",
    "- `attention`: if True, use the attention mechanism in the bottleneck of the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(C, units=100, layers=2, activation='silu', attention=True) # Example MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series, long sequences, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape of the input is `[B, C, L]`, where `L` is an unspecified sequence length. \n",
    "- `dimensions=1` is used to specify 1D CNN layers in the architecture. \n",
    "- `nf` is the base number of filters for the CNN layers.\n",
    "- `ch_mult` is used to specify the number of levels in the U-net and also is used as a multiplicative factor for the number of filters. In the example below, the CNN layers of the first level have `1 x nf`, the second have `2  nf`, etc.\n",
    "- `L` must be divisible by `2^len(ch_mult)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NCSNpp(C, dimensions=1, nf=64, ch_mult=(1, 2, 4), attention=True) # Example NCSN++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape of the input is `[B, C, H, W]`\n",
    "- Both `H` and `W` must be divisible by `2^len(ch_mult)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NCSNpp(C, nf=128, ch_mult=(2, 2, 2, 2), attention=True) # Example NCSN++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape of the input is `[B, C, H, W, D]`\n",
    "- `H`, `W` and `D` must be divisible by `2^len(ch_mult)`\n",
    "- `dimensions=3` is used to specify3D CNN layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NCSNpp(C, dimensions=3, nf=8, ch_mult=(1, 1, 2, 2), attention=True) # Example NCSN++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score-Based Model (SBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models import ScoreModel, VPSDE\n",
    "\n",
    "sde = VPSDE()\n",
    "sbm = ScoreModel(net, sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- `dataset` must be an instance of PyTorch `Dataset` or `DataLoader`. See e.g. [this tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to get started.\n",
    "- If a `Dataset` instance is provided, we wrap it automatically with a `DataLoader`. Ideally, provide `batch_size` to the `fit` method in that case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm.fit(\n",
    "    dataset,\n",
    "    epochs=100,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=16,          # Only if dataset is not a torch DataLoader\n",
    "    ema_decay=0.999,\n",
    "    checkpoints_every=10,   # Save model every 10 epochs\n",
    "    models_to_keep=1,       # Keep only the last model\n",
    "    path='/path/to/checkpoints_directory',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on HPC Cluster (Upcoming)\n",
    "\n",
    "Requires setting up the `milex-scheduler` with\n",
    "```bash\n",
    "milex-configuration\n",
    "```\n",
    "[Link to documentation]. The code below will schedule training on a cluster, possibly remote, through ssh. See [link] to configure the ssh protocol correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm.scheduled_fit(\n",
    "    dataset,\n",
    "    epochs=100,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=16,          # Only if dataset is not a torch DataLoader\n",
    "    ema_decay=0.999,\n",
    "    checkpoints_every=10,   # Save model every 10 epochs\n",
    "    models_to_keep=1,       # Keep only the last model\n",
    "    path='/path/to/checkpoints_directory',\n",
    "    time=\"03-00:00\",        # Time allocation for the job (DD-HH:MM)\n",
    "    gres=\"gpu:1\",           # Number of GPUs to allocate\n",
    "    machine=\"remote\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also allocate more than one GPU accross multiple node for heavy duty training (upcoming). The following will request 4 nodes, each with 4 GPU (assuming your compute node support 4 GPU connected with NVLinks), for a total of 16 GPU training in data parallel mode. For this type of training, the dataset must be a Dataset instance, not a DataLoader. Also, you can specify the batch size to maximize the usage of all GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm.scheduled_fit(\n",
    "    ...\n",
    "    batch_size=16*B,        # Increase the batch to be 16 times B, where B would be optimal for one GPU.\n",
    "    node=4,                 # Number of nodes to allocate\n",
    "    gres=\"gpu:4\",           # Number of GPUs to allocate\n",
    "    machine=\"remote\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "- `shape` of the input must be provided. `B` samples will be produced in parallel.\n",
    "- `steps` specifies the discretization of the SDE. It can be increased to improve the sample quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm.sample(shape=(B, C, ...), steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heavy duty sampling (upcoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm.scheduled_sample(...) # Similar to scheduled fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian Diagonal Models (HDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional SBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient fine-tuning of SBM's with LoRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caustic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
